{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97e8c94a-00a5-444d-8034-3e58e88c4d3c",
   "metadata": {},
   "source": [
    "# LSTMで体のモーションを推定する  \n",
    "## MediaPipeを使用  \n",
    "Unityのゲーム用バージョン"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f5d4ad-7542-44af-9a70-a70af862cf04",
   "metadata": {},
   "source": [
    "### 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec6901-01b5-4185-8c0a-462825fb8654",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ライブラリ読み込み \"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afec365-f9bd-4c7f-93d5-dd83cabc4433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\"\"\" GPU設定 \"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48808249-1eaa-4828-a2a1-44974f69eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''モーションデータを扱うクラス '''\n",
    "class motion_data():\n",
    "    \n",
    "    # numFrame:取り出すフレーム数、interval:取り出す間隔\n",
    "    def __init__(self, numFrame):\n",
    "        self.filelist = []       # 学習用データのファイルリスト\n",
    "        self.dataset = []        # 学習用データセット（CSVから取得）\n",
    "        self.frameTime = []      # 各フレームの時刻（CSVから取得）\n",
    "        self.feature_len = 16     # 特徴量抽出後のベクトルの長さ（extract_feature関数に合わせて変更する）\n",
    "        self.numFrame = numFrame\n",
    "        \n",
    "    # csvファイルから学習用データを取得\n",
    "    def get_csv_data(self, dirName):\n",
    "        self.filelist = []       # 学習用データのファイルリスト\n",
    "        self.dataset = []        # 学習用データセット（CSVから取得）\n",
    "        self.frameTime = []      # 各フレームの時刻（CSVから取得）\n",
    "        dirName = dirName + \"/\"\n",
    "        self.filelist = os.listdir(dirName)\n",
    "        for fileName in self.filelist:\n",
    "            fname = dirName + fileName\n",
    "            # 以下はnpyファイルを読み込むとき\n",
    "            #newdata = np.load(fname)\n",
    "            # 以下はcsvファイルを読み込むとき\n",
    "            csv_data = np.loadtxt(fname, delimiter=',', skiprows=2)    # skiprowsには先頭の飛ばす行数を指定\n",
    "            newframeTime = csv_data[:, 1]\n",
    "            newdata = np.array(csv_data[:, 2:])\n",
    "            #newdata = newdata.reshape([csv_data.shape[0], csv_data.shape[1] // 3, 3])\n",
    "            self.dataset.extend([newdata])\n",
    "            self.frameTime.extend([newframeTime])\n",
    "            \n",
    "    def angle_vec2(self, v1, v2):\n",
    "        return np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "    \n",
    "    # p->p1とp->p2のベクトルの前額面の角度・矢状面の角度を返す\n",
    "    def frontal_satittal_angle(self, p, p1, p2):\n",
    "        v1, v2 = p1 - p, p2 - p\n",
    "        v1_f, v1_s = np.array([v1[0], v1[1]]), np.array([v1[2], v1[1]])\n",
    "        v2_f, v2_s = np.array([v2[0], v2[1]]), np.array([v2[2], v2[1]])\n",
    "        frontal = self.angle_vec2(v1_f, v2_f)\n",
    "        sagittal = self.angle_vec2(v1_s, v2_s)\n",
    "        return [frontal, sagittal]\n",
    "            \n",
    "    # 特徴量抽出を行う関数\n",
    "    # 圧縮後の次元数がself.feature_lenになるように注意\n",
    "    # 引数：1時刻のデータ\n",
    "    def extract_feature(self, data):\n",
    "        # ここで何らかの特徴量抽出の処理をする\n",
    "        vec3_data = data.reshape([len(data) // 3, 3])\n",
    "        \n",
    "        lhip = vec3_data[23]\n",
    "        lshoulder = vec3_data[11]\n",
    "        lknee = vec3_data[25]\n",
    "        lelbow = vec3_data[13]\n",
    "        lankle = vec3_data[27]\n",
    "        lwrist = vec3_data[15]\n",
    "        lhip_f, lhip_s = self.frontal_satittal_angle(lhip, lshoulder, lknee)\n",
    "        lshoulder_f, lshouler_s = self.frontal_satittal_angle(lshoulder, lhip, lelbow)\n",
    "        lknee_f, lknee_s = self.frontal_satittal_angle(lknee, lhip, lankle)\n",
    "        lelbow_f, lelbow_s = self.frontal_satittal_angle(lelbow, lshoulder, lwrist)\n",
    "        \n",
    "        rhip = vec3_data[24]\n",
    "        rshoulder = vec3_data[12]\n",
    "        rknee = vec3_data[26]\n",
    "        relbow = vec3_data[14]\n",
    "        rankle = vec3_data[28]\n",
    "        rwrist = vec3_data[16]\n",
    "        rhip_f, rhip_s = self.frontal_satittal_angle(rhip, rshoulder, rknee)\n",
    "        rshoulder_f, rshouler_s = self.frontal_satittal_angle(rshoulder, rhip, relbow)\n",
    "        rknee_f, rknee_s = self.frontal_satittal_angle(rknee, rhip, rankle)\n",
    "        relbow_f, relbow_s = self.frontal_satittal_angle(relbow, rshoulder, rwrist)\n",
    "        \n",
    "        retData = [lhip_f, lhip_s, lshoulder_f, lshouler_s, lknee_f, lknee_s, lelbow_f, lelbow_s,\n",
    "                   rhip_f, rhip_s, rshoulder_f, rshouler_s, rknee_f, rknee_s, relbow_f, relbow_s]\n",
    "        retData =  np.array(retData)\n",
    "        \n",
    "        ''' 以下はカメラ1台のとき\n",
    "        vec2_data = vec3_data[:, 0:2]    # z座標は使わない\n",
    "        \n",
    "        lhip = vec2_data[23]\n",
    "        lshoulder = vec2_data[11]\n",
    "        lknee = vec2_data[25]\n",
    "        lelbow = vec2_data[13]\n",
    "        lankle = vec2_data[27]\n",
    "        lhip_shoulder = lshoulder - lhip\n",
    "        lhip_knee = lknee - lhip\n",
    "        lhip_angle = self.angle_vec2(lhip_shoulder, lhip_knee)\n",
    "        lshoulder_elbow = lelbow - lshoulder\n",
    "        lshoulder_angle = self.angle_vec2(-lhip_shoulder, lshoulder_elbow)\n",
    "        lknee_ankle = lankle - lknee\n",
    "        lknee_angle = self.angle_vec2(lknee_ankle, -lhip_knee)\n",
    "        \n",
    "        rhip = vec2_data[24]\n",
    "        rshoulder = vec2_data[12]\n",
    "        rknee = vec2_data[26]\n",
    "        relbow = vec2_data[14]\n",
    "        rankle = vec2_data[28]\n",
    "        rhip_shoulder = rshoulder - rhip\n",
    "        rhip_knee = rknee - rhip\n",
    "        rhip_angle = self.angle_vec2(rhip_shoulder, rhip_knee)\n",
    "        rshoulder_elbow = relbow - rshoulder\n",
    "        rshoulder_angle = self.angle_vec2(-rhip_shoulder, rshoulder_elbow)\n",
    "        rknee_ankle = rankle - rknee\n",
    "        rknee_angle = self.angle_vec2(rknee_ankle, -rhip_knee)\n",
    "        \n",
    "        retData = np.array([lhip_angle, lshoulder_angle, lknee_angle, rhip_angle, rshoulder_angle, rknee_angle])\n",
    "        '''\n",
    "        return retData\n",
    "    \n",
    "    # モーションデータの中から引数intervalの間隔でデータを取り出し\n",
    "    # それを複数パターンでndarray化して返す\n",
    "    def split_interval_data(self, data, interval):\n",
    "        numData = len(data) - (interval * (self.numFrame - 1))\n",
    "        if (numData <= 0):\n",
    "            return np.empty((0, self.numFrame, self.feature_len))\n",
    "        retData = np.empty((numData, self.numFrame, self.feature_len))\n",
    "        for i in range(numData):\n",
    "            splitData = np.empty((self.numFrame, self.feature_len))    # 分割データの保存先\n",
    "            for j in range(self.numFrame):\n",
    "                splitData[j] = self.extract_feature(data[i + interval * j])\n",
    "            retData[i] = splitData\n",
    "        return retData\n",
    "\n",
    "    # モーションデータをminIntervalから最大のインターバルで間引きしたnumFrameの長さのデータセットを返す\n",
    "    # これにより様々な速さのモーションデータセットを得る\n",
    "    def get_interval_data(self, minInterval, correctID, maxInterval=-1):\n",
    "        numMotion = len(self.dataset)\n",
    "        dataset = np.empty((0, self.numFrame, self.feature_len))\n",
    "        for i in range(numMotion):    # すべてのモーションデータセットに対して\n",
    "            if maxInterval < 0:\n",
    "                maxInterval = (len(self.dataset[i]) - 1) // (self.numFrame - 1)\n",
    "            for interval in range(minInterval, maxInterval + 1):\n",
    "                splitdata = self.split_interval_data(self.dataset[i], interval)\n",
    "                dataset = np.concatenate([dataset, splitdata])\n",
    "        correct_data = np.ones(len(dataset), dtype=np.int64) * correctID\n",
    "        return [dataset, correct_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29163d8d-9c8d-4f52-a342-0b7e0dfae0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モーションデータのオブジェクト\n",
    "numFrame = 5     # モーションデータのフレーム数\n",
    "interval = 2     # モーションデータを何フレームに一度取り出すか\n",
    "maxInterval = 10    # 1モーションデータの最大のフレーム数\n",
    "motion_count = 4\n",
    "m = []\n",
    "for i in range(motion_count):\n",
    "    m.append(motion_data(numFrame))     # モーションのデータオブジェクト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bdae23-8374-4904-b725-57563f9a5d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LSTMのモデルクラス \"\"\"\n",
    "class Net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.feature_size = 16         # 特徴量x(t)の次元\n",
    "        self.hidden_layer_size = 24    # 隠れ層のサイズ\n",
    "        self.lstm_layers = 1           # LSTMのレイヤー数　(LSTMを何層重ねるか)\n",
    "        self.output_size = 4           # 出力層のサイズ\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(self.feature_size, \n",
    "                                  self.hidden_layer_size, \n",
    "                                  num_layers = self.lstm_layers)\n",
    "        \n",
    "        self.fc = torch.nn.Linear(self.hidden_layer_size, self.output_size)\n",
    "        \n",
    "        self.softmax = torch.nn.Softmax(dim=1)  # softmax関数で配列の要素の総和が1になるように変換\n",
    "        \n",
    "    def init_hidden_cell(self, batch_size): # LSTMの隠れ層 hidden と記憶セル cell を初期化\n",
    "        hedden = torch.zeros(self.lstm_layers, batch_size, self.hidden_layer_size)\n",
    "        cell = torch.zeros(self.lstm_layers, batch_size, self.hidden_layer_size)        \n",
    "        return (hedden, cell)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        self.hidden_cell = self.init_hidden_cell(batch_size)\n",
    "        x = x.permute(1, 0, 2)                                   # (Batch, Seqence, Feature) -> (Seqence , Batch, Feature)\n",
    "        \n",
    "        lstm_out, (h_n, c_n) = self.lstm(x, self.hidden_cell)    # LSTMの入力データのShapeは(Seqence, Batch, Feature)\n",
    "                                                                 # (h_n) のShapeは (num_layers, batch, hidden_size)\n",
    "        x = h_n[-1,:,:]                                          # lstm_layersの最後のレイヤーを取り出す  (B, h)\n",
    "                                                                 # lstm_outの最後尾と同じ？\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ab9924-707e-4463-9978-a26874aa5e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較用にDNNも用意\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):  # コンストラクタ\n",
    "        super().__init__()  # 親クラスのコンストラクタを呼び出し\n",
    "        feature_size = 16         # 特徴量x(t)の次元\n",
    "        time_size = 5             # 時系列の長さ\n",
    "        hidden_layer_size1 = 24   # 隠れ層のサイズ\n",
    "        hidden_layer_size2 = 24\n",
    "        output_size = 4           # 出力層のサイズ\n",
    "        \n",
    "        self.input_size = feature_size * time_size\n",
    "        fc1 = nn.Linear(self.input_size, hidden_layer_size1)  # 入力層 -> 隠れ層\n",
    "        sig1 = nn.Sigmoid()  # 活性化関数(シグモイド)、行列計算？\n",
    "        fc2 = nn.Linear(hidden_layer_size1, hidden_layer_size2)  # 入力層 -> 隠れ層\n",
    "        sig2 = nn.Sigmoid()\n",
    "        fc3 = nn.Linear(hidden_layer_size2, output_size)  # 隠れ層 -> 出力層\n",
    "        softmax = torch.nn.Softmax(dim=1)  # softmax関数で配列の要素の総和が1になるように変換\n",
    "        self.model_info = nn.ModuleList([fc1, sig1, fc2, sig2, fc3, softmax])  # メンバ変数に登録\n",
    "\n",
    "    # forwardメソッドは親クラスの__call__メソッドから呼び出される、__call__は呼び出し可能オブジェクト\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape([batch_size, self.input_size])\n",
    "        for i in range(len(self.model_info)):\n",
    "            x = self.model_info[i](x)    # self.model_infoをすべて実行\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42afd015-3499-4c2b-8259-b1e587cac294",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f19b22-50f4-4e08-94fe-5d27ea487319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\"\"\" データの準備 \"\"\"\n",
    "# 訓練データ（とテストデータ）から一度に何個のデータを読み込むかを指定する値\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "# モーションデータを取り出して分割\n",
    "m[0].get_csv_data(\"./stop\")\n",
    "m[1].get_csv_data(\"./walk\")\n",
    "m[2].get_csv_data(\"./jump\")\n",
    "m[3].get_csv_data(\"./run\")\n",
    "\n",
    "input_data = []\n",
    "correct_data = []\n",
    "data_size = []\n",
    "for i in range(motion_count):\n",
    "    print(\"Motion:%d Processing data...\" % i)\n",
    "    ms, mc = m[i].get_interval_data(interval, i, maxInterval=20)\n",
    "    input_data.append(ms)\n",
    "    correct_data.append(mc)\n",
    "    data_size.append(len(mc))\n",
    "print(\"Data size: \" + str(data_size))\n",
    "    \n",
    "# 学習データの数をモーションごとにそろえる（少ないデータの数に合わせる）\n",
    "min_data_size = min(data_size)\n",
    "for i in range(motion_count):\n",
    "    idx = random.sample(list(range(data_size[i])), k=min_data_size)\n",
    "    input_data[i] = (input_data[i])[idx]\n",
    "    correct_data[i] = (correct_data[i])[idx]\n",
    "\n",
    "# 学習用モーションデータと正解ラベルのデータを作成\n",
    "input_data = np.concatenate(input_data)    # リストの要素同士を結合\n",
    "correct_data = np.concatenate(correct_data)\n",
    "\n",
    "np.save('input_data', input_data)\n",
    "np.save('correct_data', correct_data)\n",
    "print(input_data * 180 / np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d08d2c-b5d9-4f7b-8aeb-c738d5b88db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.load('input_data.npy')\n",
    "correct_data = np.load('correct_data.npy')\n",
    "\n",
    "# テンソルに変換\n",
    "input_data = torch.FloatTensor(input_data)\n",
    "correct_data = torch.LongTensor(correct_data)\n",
    "#input_data = torch.cuda.FloatTensor(input_data)\n",
    "#correct_data = torch.cuda.LongTensor(correct_data)\n",
    "\n",
    "# データセットの準備\n",
    "dataset = torch.utils.data.TensorDataset(input_data, correct_data)\n",
    "# 学習用データと検証用データに分割\n",
    "train_size = int(0.8 * len(dataset))    # 学習用データのサイズ（全体の8割）\n",
    "test_size = len(dataset) - train_size    # 検証用データのサイズ\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "#print(vars(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c1e55b4-2283-4a42-819a-eb354b09b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 設定 \"\"\"\n",
    "# モデル作成\n",
    "net = Net().to(device)\n",
    "#net = DNN().to(device)\n",
    "# 損失関数（計算結果と正解ラベルの誤差を比較、それを基に最適化）\n",
    "criterion = torch.nn.CrossEntropyLoss()  # CrossEntropyLossは損失関数に多クラス分類でよく使われる\n",
    "# 最適化アルゴリズム\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  # SGD（確率的勾配降下法）\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58da21e5-f5d6-4cbb-ae7e-e190b5c6092d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1, data: 2000, running_loss: 1.243\n",
      "#1, data: 4000, running_loss: 0.947\n",
      "#1, data: 6000, running_loss: 0.871\n",
      "#1, data: 8000, running_loss: 0.825\n",
      "#1, data: 10000, running_loss: 0.792\n",
      "#1, data: 12000, running_loss: 0.780\n",
      "#1, data: 14000, running_loss: 0.769\n",
      "#1, data: 16000, running_loss: 0.768\n",
      "#1, data: 18000, running_loss: 0.761\n",
      "#1, data: 20000, running_loss: 0.760\n",
      "#2, data: 2000, running_loss: 0.762\n",
      "#2, data: 4000, running_loss: 0.761\n",
      "#2, data: 6000, running_loss: 0.761\n",
      "#2, data: 8000, running_loss: 0.763\n",
      "#2, data: 10000, running_loss: 0.753\n",
      "#2, data: 12000, running_loss: 0.754\n",
      "#2, data: 14000, running_loss: 0.754\n",
      "#2, data: 16000, running_loss: 0.751\n",
      "#2, data: 18000, running_loss: 0.751\n",
      "#2, data: 20000, running_loss: 0.755\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 学習 \"\"\"\n",
    "EPOCHS = 2  # すべての入力に対してn回実行\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    running_loss = 0.0  # 平均値出力用\n",
    "    for count, item in enumerate(trainloader, 1):  # BATCH_SIZEごとに実行するため、このときcountの値を増やす\n",
    "        inputs, labels = item  # trainloader経由でデータを20個取り出す\n",
    "        \n",
    "        # CUDAで使えるようキャスト\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # 重みとバイアスの更新で内部的に使用するデータセット\n",
    "        \n",
    "        # Runs the forward pass with autocasting.\n",
    "        outputs = net(inputs)  # ニューラルネットワークにデータを入力\n",
    "        \n",
    "        loss = criterion(outputs, labels)  # 正解ラベルとの比較\n",
    "\n",
    "        loss.backward()  # 誤差逆伝播\n",
    "        optimizer.step()  # 重みとバイアスの更新\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        times = 100\n",
    "        if count % times == 0:\n",
    "            print(f'#{epoch}, data: {count * BATCH_SIZE}, running_loss: {running_loss / times:1.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "torch.save(net, 'body1.pth')\n",
    "print('Training Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb4515ba-dac4-4489-912c-8248f926bf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 0])\n",
      "tensor([0, 0, 0, 1, 0])\n",
      "correct: 5044, accuracy: 5044 / 5067 = 99.54608249457273 %\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 結果出力 \"\"\"\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "print(predicted)\n",
    "print(labels)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += len(outputs)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'correct: {correct}, accuracy: {correct} / {total} = {100 * correct / total} %')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c44c7-6759-4302-b274-54cd3114ef83",
   "metadata": {},
   "source": [
    "### リアルタイム認識"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5888fd3e-bc29-46bd-b226-49d96b212e37",
   "metadata": {},
   "source": [
    "#### UDPの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d0e50-1544-4e99-9d01-2230927666b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from socket import *\n",
    "\n",
    "class udp_client:\n",
    "    def __init__(self):\n",
    "        # 送信側IP\n",
    "        SrcIP = \"127.0.0.1\"    # localhost\n",
    "        # 送信側ポート番号\n",
    "        SrcPort = 11111\n",
    "        # 送信側アドレスをtupleに格納\n",
    "        self.SrcAddr = (SrcIP, SrcPort)\n",
    "\n",
    "        # 受信側アドレスの設定\n",
    "        # 受信側IP\n",
    "        DstIP = \"127.0.0.1\"\n",
    "        # 受信側ポート番号\n",
    "        DstPort = 22222\n",
    "        # 受信側アドレスをtupleに格納\n",
    "        self.DstAddr = (DstIP, DstPort)\n",
    "\n",
    "        # ソケット作成\n",
    "        self.udpClntSock = socket(AF_INET, SOCK_DGRAM)\n",
    "        # 送信側アドレスでソケットを設定\n",
    "        self.udpClntSock.bind(self.SrcAddr)\n",
    "        \n",
    "    \n",
    "    def __delete__(self):\n",
    "        del self.udpClntSock\n",
    "        \n",
    "        \n",
    "    def send(self, data):\n",
    "        # バイナリに変換\n",
    "        data = data.encode('utf-8')\n",
    "\n",
    "        # 受信側アドレスに送信\n",
    "        self.udpClntSock.sendto(data, self.DstAddr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab2d40-bc94-426c-91ff-660415491d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8748baca-d5ee-46f6-92a6-4a8759e91d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" モーションの間引き \"\"\"\n",
    "class motion_analyze():\n",
    "    # motion_lenはnumFrame, intervalので抽出して足りるフレーム数\n",
    "    def __init__(self, motion_len):\n",
    "        self.vec_size = 33\n",
    "        self.data = np.zeros((motion_len, self.vec_size * 3), dtype=np.float32)\n",
    "    \n",
    "    def LandmarkToArray(self, landmark):\n",
    "        return np.array([landmark.x, landmark.y, landmark.z])\n",
    "    \n",
    "    '''\n",
    "    # 矢状面だけのとき\n",
    "    def record_old(self, results):\n",
    "        if results.pose_landmarks is None:\n",
    "            return None\n",
    "        \n",
    "        data = np.empty((self.vec_size, 3), dtype=np.float32)\n",
    "        for i in range(self.vec_size):\n",
    "            data[i] = self.LandmarkToArray(results.pose_landmarks.landmark[i])\n",
    "        data = data.reshape(self.vec_size * 3)\n",
    "        \n",
    "        self.data = np.roll(self.data, -1, axis=0)\n",
    "        self.data[-1] = data\n",
    "        #self.time_ary.append(time.perf_counter())\n",
    "        \n",
    "        return self.data\n",
    "    '''\n",
    "    \n",
    "    def record(self, results_front, results_sagittal):\n",
    "        if results_front.pose_landmarks is None:\n",
    "            return None\n",
    "        if results_sagittal.pose_landmarks is None:\n",
    "            return None\n",
    "        \n",
    "        data = np.empty((self.vec_size, 3), dtype=np.float32)\n",
    "        for i in range(self.vec_size):\n",
    "            front = self.LandmarkToArray(results_front.pose_landmarks.landmark[i])\n",
    "            sagittal = self.LandmarkToArray(results_sagittal.pose_landmarks.landmark[i])\n",
    "            data[i][0] = front[0]    # x座標は前額面から\n",
    "            #data[i][1] = (front[1] + sagittal[1]) / 2    # y座標は矢状面と前額面のy座標の平均\n",
    "            data[i][1] = sagittal[1]    # y座標はとりあえず矢状面から取得\n",
    "            data[i][2] = sagittal[0]    # z座標は矢状面から\n",
    "        data = data.reshape(self.vec_size * 3)\n",
    "        \n",
    "        self.data = np.roll(self.data, -1, axis=0)\n",
    "        self.data[-1] = data\n",
    "\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4160e2c-161a-4f85-be9b-e214e6a1af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" MediaPipe複数台 \"\"\"\n",
    "class my_mediapipe():\n",
    "\n",
    "    def __init__(self, capture=0, window_name=\"MediaPipe\"):\n",
    "        self.windowName = window_name\n",
    "        # For webcam input:\n",
    "        self.cap = cv2.VideoCapture(capture)\n",
    "\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mp_drawing_styles = mp.solutions.drawing_styles\n",
    "        self.mp_holistic = mp.solutions.holistic\n",
    "\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mesh_drawing_spec = self.mp_drawing.DrawingSpec(thickness=2,  color=(0,255,0))\n",
    "        self.mark_drawing_spec = self.mp_drawing.DrawingSpec(thickness=3,  circle_radius=3, color=(0,0,255))\n",
    "\n",
    "        self.holistic = self.mp_holistic.Holistic(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5)\n",
    "        \n",
    "        self.w = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))         # カメラの横幅を取得\n",
    "        self.h = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))        # カメラの縦幅を取得\n",
    "\n",
    "\n",
    "    # 終了時に必ず呼び出し\n",
    "    def close(self):\n",
    "        self.holistic.close()    # withを使わない代わり？\n",
    "        self.cap.release()\n",
    "        #cv2.destroyAllWindows()\n",
    "        \n",
    "        \n",
    "    # with文が無くても動く？\n",
    "    def loop(self, show_caputer=True):\n",
    "        if self.cap.isOpened():\n",
    "            success, image = self.cap.read()\n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                # If loading a video, use 'break' instead of 'continue'.\n",
    "                return\n",
    "            \n",
    "            image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)           # 時計回り90度回転\n",
    "            # To improve performance, optionally mark the image as not writeable to\n",
    "            # pass by reference.\n",
    "            image.flags.writeable = False\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            results = self.holistic.process(image)\n",
    "\n",
    "            # Draw landmark annotation on the image.\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            if (show_caputer is False):\n",
    "                image = np.zeros([self.w, self.h, 3])\n",
    "\n",
    "            self.mp_drawing.draw_landmarks(    # ポーズの特徴\n",
    "                image,\n",
    "                results.pose_landmarks,\n",
    "                self.mp_holistic.POSE_CONNECTIONS,\n",
    "                landmark_drawing_spec=self.mp_drawing_styles\n",
    "                .get_default_pose_landmarks_style())\n",
    "\n",
    "            # Flip the image horizontally for a selfie-view display.\n",
    "            cv2.imshow(self.windowName, cv2.flip(image, 1))\n",
    "\n",
    "            return results\n",
    "\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e28574-a5fd-4a06-b419-93ac1e9290aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "motion_class = motion_analyze(numFrame)\n",
    "udp = udp_client()\n",
    "\n",
    "''' 顔とポーズを取得 '''\n",
    "# https://google.github.io/mediapipe/solutions/holistic\n",
    "\n",
    "\"\"\" GPU設定 \"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\"\"\" 識別 \"\"\"\n",
    "net = torch.load('body1.pth')\n",
    "\n",
    "mediapipe1 = my_mediapipe(0, window_name=\"MediaPipe1\")    # 正面から\n",
    "mediapipe2 = my_mediapipe(1, window_name=\"MediaPipe2\")    # 横から\n",
    "\n",
    "while(True):\n",
    "    clear_output(wait=True)\n",
    "    results1 = mediapipe1.loop(False)\n",
    "    results2 = mediapipe2.loop(False)\n",
    "    \n",
    "    data = motion_class.record(results1, results2)\n",
    "    if data is not None:\n",
    "        motion = m[0].split_interval_data(data, 1)[0]\n",
    "        motion = motion.reshape((1, numFrame, len(motion[0])))\n",
    "        motion = torch.FloatTensor(motion)\n",
    "        outputs = net(motion)  # ニューラルネットワークにデータを入力\n",
    "        value, predicted_idx = torch.max(outputs, 1)\n",
    "        answer = int(predicted_idx[0])\n",
    "        print(value)\n",
    "        if (answer == 0):\n",
    "            print(\"stop\")\n",
    "            udp.send(\"stop\")\n",
    "        elif (answer == 1):\n",
    "            print(\"walk\")\n",
    "            udp.send(\"walk\")\n",
    "        elif (answer == 2):\n",
    "            print(\"jump\")\n",
    "            udp.send(\"jump\")\n",
    "        elif (answer == 3):\n",
    "            print(\"run\")\n",
    "            udp.send(\"run\")\n",
    "        else:\n",
    "            print(\"error\")\n",
    "        \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27:    # Escで終了\n",
    "        break\n",
    "\n",
    "mediapipe1.close()\n",
    "mediapipe2.close()\n",
    "cv2.destroyAllWindows()\n",
    "del udp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7083d3-ef6e-4f64-8832-95026c8a5214",
   "metadata": {},
   "outputs": [],
   "source": [
    "mediapipe1.close()\n",
    "mediapipe2.close()\n",
    "cv2.destroyAllWindows()\n",
    "del udp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2603116-e8bb-4f3a-9fa2-486fd7113306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
